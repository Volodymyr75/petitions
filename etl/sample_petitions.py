"""
Sample petition IDs from different ranges to understand field availability and statuses
"""
import requests
from bs4 import BeautifulSoup
import time
import json
from collections import Counter

BASE_URL = "https://petition.president.gov.ua/petition/"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# ID ranges to sample
RANGES = [
    (1, 20),
    (1001, 1020),
    (5001, 5020),
    (60001, 60020),
    (100001, 100020),
    (150001, 150020)
]

def extract_petition_data(pet_id):
    """Extract all available fields from a petition page"""
    url = f"{BASE_URL}{pet_id}"
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        
        # Check for 404
        if "404" in resp.text or "–Ω–µ —ñ—Å–Ω—É—î" in resp.text or resp.status_code != 200:
            return None
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # Title
        h1 = soup.find('h1')
        if not h1:
            return None
        
        data = {
            'id': pet_id,
            'title': h1.get_text(strip=True),
            'fields_found': []
        }
        
        # Number
        num_tag = soup.find(class_='pet_number')
        if num_tag:
            data['number'] = num_tag.get_text(strip=True)
            data['fields_found'].append('number')
        
        # All date/author fields
        date_tags = soup.find_all(class_='pet_date')
        for dt in date_tags:
            text = dt.get_text(strip=True)
            if "–ê–≤—Ç–æ—Ä" in text or "—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä" in text:
                data['author'] = text
                data['fields_found'].append('author')
            elif "–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è" in text:
                data['date_published'] = text
                data['fields_found'].append('date_published')
        
        # Status - check all possible indicators
        status_found = []
        if soup.find(string=lambda t: t and "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤" in t):
            status_found.append("–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤")
        if soup.find(string=lambda t: t and "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ" in t):
            status_found.append("–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ")
        if soup.find(string=lambda t: t and "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é" in t):
            status_found.append("–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é")
        if soup.find(string=lambda t: t and "–ê—Ä—Ö—ñ–≤" in t):
            status_found.append("–ê—Ä—Ö—ñ–≤")
        if soup.find(string=lambda t: t and "–û—á—ñ–∫—É—î" in t):
            status_found.append("–û—á—ñ–∫—É—î –Ω–∞ —Ä–æ–∑–≥–ª—è–¥")
            
        data['status'] = status_found[0] if status_found else "Unknown"
        data['fields_found'].append('status')
        
        # Text
        article = soup.find(class_='article')
        if article:
            data['text_length'] = len(article.get_text())
            data['fields_found'].append('text')
        
        # Answer
        answer_tab = soup.find(string=lambda t: t and "–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–µ—Ç–∏—Ü—ñ—é" in t)
        if answer_tab:
            data['has_answer'] = True
            data['fields_found'].append('answer')
        
        return data
        
    except Exception as e:
        print(f"  Error: {e}")
        return None


# Collect data
all_data = []
status_counter = Counter()

print("üîç Sampling petition IDs across different ranges...\n")

for start, end in RANGES:
    print(f"\n{'='*60}")
    print(f"Range: {start}-{end}")
    print('='*60)
    
    valid_count = 0
    
    for pet_id in range(start, end + 1):
        data = extract_petition_data(pet_id)
        
        if data:
            valid_count += 1
            all_data.append(data)
            status_counter[data['status']] += 1
            print(f"  ‚úÖ {pet_id}: {data['status']}")
        else:
            print(f"  ‚ùå {pet_id}: 404/Invalid")
        
        time.sleep(0.3)  # Be polite
    
    print(f"\n–ó–Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª—ñ–¥–Ω–∏—Ö: {valid_count}/{end-start+1}")

# Analysis
print("\n\n" + "="*60)
print("üìä –ê–ù–ê–õ–Ü–ó –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í")
print("="*60)

print("\n1. –°–¢–ê–¢–£–°–ò –ó–ù–ê–ô–î–ï–ù–Ü:")
for status, count in status_counter.most_common():
    print(f"   {status:30} = {count} –ø–µ—Ç–∏—Ü—ñ–π")

print("\n2. –ü–û–õ–Ø –ó–ù–ê–ô–î–ï–ù–Ü –£ –ü–ï–¢–ò–¶–Ü–Ø–•:")
all_fields = set()
for d in all_data:
    all_fields.update(d.get('fields_found', []))
print(f"   {', '.join(sorted(all_fields))}")

print("\n3. –ß–ò –Ñ '–ê–†–•–Ü–í' –Ø–ö –°–¢–ê–¢–£–°?")
if any('–ê—Ä—Ö—ñ–≤' in d['status'] for d in all_data):
    archive_count = status_counter.get('–ê—Ä—Ö—ñ–≤', 0)
    print(f"   ‚úÖ –¢–ê–ö! –ó–Ω–∞–π–¥–µ–Ω–æ {archive_count} –ø–µ—Ç–∏—Ü—ñ–π –∑—ñ —Å—Ç–∞—Ç—É—Å–æ–º '–ê—Ä—Ö—ñ–≤'")
    
    # Show examples
    print("\n   –ü—Ä–∏–∫–ª–∞–¥–∏ –∞—Ä—Ö—ñ–≤–Ω–∏—Ö –ø–µ—Ç–∏—Ü—ñ–π:")
    for d in all_data:
        if '–ê—Ä—Ö—ñ–≤' in d['status']:
            print(f"     - ID {d['id']}: {d['title'][:50]}...")
            if len([x for x in all_data if '–ê—Ä—Ö—ñ–≤' in x['status']]) >= 3:
                break
else:
    print("   ‚ùå –ù–Ü! –ñ–æ–¥–Ω–æ—ó –ø–µ—Ç–∏—Ü—ñ—ó –∑—ñ —Å—Ç–∞—Ç—É—Å–æ–º '–ê—Ä—Ö—ñ–≤' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

# Save to file
with open('etl/petition_sampling_results.json', 'w', encoding='utf-8') as f:
    json.dump(all_data, f, indent=2, ensure_ascii=False)

print(f"\n‚úÖ –î–µ—Ç–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: etl/petition_sampling_results.json")
print(f"   –í—Å—å–æ–≥–æ –∑—ñ–±—Ä–∞–Ω–æ: {len(all_data)} –≤–∞–ª—ñ–¥–Ω–∏—Ö –ø–µ—Ç–∏—Ü—ñ–π")
