import requests
from bs4 import BeautifulSoup
import duckdb
import time
import random
import argparse
from datetime import datetime


BASE_URL = "https://petition.president.gov.ua/petition/"
HEADERS = {
    "User-Agent": "PetitionsResearchBot/1.0 (+contact: your-email@example.com)"
}
DB_FILE = "petitions.duckdb"

# –ì–ª–æ–±–∞–ª—å–Ω–∞ —Å–µ—Å—ñ—è
session = requests.Session()


def polite_sleep(iteration):
    """–ë—ñ–ª—å—à –º'—è–∫—ñ –∑–∞—Ç—Ä–∏–º–∫–∏ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏"""
    # –ë–∞–∑–æ–≤–∞ ¬´–ª—é–¥—Å—å–∫–∞¬ª –ø–∞—É–∑–∞ 2‚Äì6 —Å–µ–∫—É–Ω–¥
    time.sleep(random.uniform(0.7, 1.4))

    # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –¥–æ–≤–≥–∞ –ø–∞—É–∑–∞ –∫–æ–∂–Ω—ñ 50 –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–∏—Ö ID
    if iteration % 50 == 0:
        extra = random.uniform(3.0, 6.0)
        print(f"‚è≥ –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–∞—É–∑–∞ {extra:.1f} —Å –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è")
        time.sleep(extra)


def extract_petition_data(pet_id):
    """–í–∏—Ç—è–≥—É—î –≤—Å—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ –¥–∞–Ω—ñ –∑ –ø–µ—Ç–∏—Ü—ñ—ó –∑ —Ä–µ—Ç—Ä–∞—è–º–∏ —Ç–∞ –±–µ–∫–æ—Ñ–æ–º"""
    url = f"{BASE_URL}{pet_id}"

    max_attempts = 3
    attempt = 0

    while attempt < max_attempts:
        try:
            resp = session.get(url, headers=HEADERS, timeout=15)

            # –Ø–≤–Ω–µ –æ–±–º–µ–∂–µ–Ω–Ω—è: 429/503 ‚Üí –¥–æ–≤—à–∞ –ø–∞—É–∑–∞ –∑ –±–µ–∫–æ—Ñ–æ–º
            if resp.status_code in (429, 503):
                wait = 30 * (attempt + 1)
                print(f"‚è≥ Rate limit {resp.status_code} –Ω–∞ ID {pet_id}, sleep {wait}s (attempt {attempt+1})")
                time.sleep(wait)
                attempt += 1
                continue

            if resp.status_code != 200:
                return None

            # Check for 404/redirect/–Ω–µ —ñ—Å–Ω—É—î
            if "404" in resp.text or "–Ω–µ —ñ—Å–Ω—É—î" in resp.text or "Redirecting" in resp.text:
                return None

            soup = BeautifulSoup(resp.text, 'html.parser')

            # Title (–∫—Ä–∏—Ç–∏—á–Ω–æ)
            h1 = soup.find('h1')
            if not h1:
                return None

            data = {
                'source': 'president',
                'id': str(pet_id),
                'title': h1.get_text(strip=True)
            }

            # Number
            num_tag = soup.find(class_='pet_number')
            data['number'] = num_tag.get_text(strip=True) if num_tag else None

            # Date + Author
            date_tags = soup.find_all(class_='pet_date')
            data['author'] = None
            data['date'] = None

            for dt in date_tags:
                text = dt.get_text(strip=True)
                if "–ê–≤—Ç–æ—Ä" in text or "—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä" in text:
                    if ":" in text:
                        data['author'] = text.split(":", 1)[1].strip()
                    else:
                        data['author'] = text.replace("–ê–≤—Ç–æ—Ä (—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä)", "").strip()
                elif "–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è" in text:
                    if ":" in text:
                        data['date'] = text.split(":", 1)[1].strip()
                    else:
                        data['date'] = text.replace("–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è", "").strip()

            # Status
            data['status'] = "Unknown"
            if soup.find(string=lambda t: t and "–ê—Ä—Ö—ñ–≤" in t):
                data['status'] = "–ê—Ä—Ö—ñ–≤"
            elif soup.find(string=lambda t: t and "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é" in t):
                data['status'] = "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é"
            elif soup.find(string=lambda t: t and "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ" in t):
                data['status'] = "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ"
            elif soup.find(string=lambda t: t and "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤" in t):
                data['status'] = "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤"

            # Votes
            votes_graph = soup.find(class_='petition_votes_graph')
            if votes_graph:
                try:
                    data['votes'] = int(votes_graph.get('data-votes', 0))
                except Exception:
                    data['votes'] = None
            else:
                data['votes'] = None

            # URL
            data['url'] = url

            # Text length
            article = soup.find(class_='article')
            data['text_length'] = len(article.get_text()) if article else None

            # Has answer
            answer_tab = soup.find(string=lambda t: t and "–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–µ—Ç–∏—Ü—ñ—é" in t)
            data['has_answer'] = answer_tab is not None

            return data

        except Exception as e:
            attempt += 1
            wait = 5 * attempt
            print(f"  üí• Error scraping {pet_id}: {e}, retry in {wait}s (attempt {attempt})")
            time.sleep(wait)

    # –Ø–∫—â–æ –≤—Å—ñ —Å–ø—Ä–æ–±–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—è
    return None


def load_existing_ids(con):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤—Å—ñ existing petition IDs –≤ –ø–∞–º'—è—Ç—å (—à–≤–∏–¥–∫–æ)"""
    result = con.execute("""
        SELECT external_id FROM petitions WHERE source='president'
    """).fetchall()
    return set(row[0] for row in result)


def insert_new(con, petition):
    """INSERT –Ω–æ–≤–æ—ó –ø–µ—Ç–∏—Ü—ñ—ó"""
    con.execute("""
        INSERT INTO petitions (source, external_id, number, title, date, status, votes, url, author, text_length, has_answer)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        petition['source'],
        petition['id'],
        petition.get('number'),
        petition.get('title'),
        petition.get('date'),
        petition.get('status'),
        petition.get('votes'),
        petition.get('url'),
        petition.get('author'),
        petition.get('text_length'),
        petition.get('has_answer')
    ))


def update_existing(con, petition):
    """UPDATE —ñ—Å–Ω—É—é—á–æ—ó –ø–µ—Ç–∏—Ü—ñ—ó (—Ç—ñ–ª—å–∫–∏ NULL –ø–æ–ª—è)"""
    existing = con.execute("""
        SELECT number, title, date, status, votes, url, author, text_length, has_answer
        FROM petitions
        WHERE source=? AND external_id=?
    """, (petition['source'], petition['id'])).fetchone()

    if not existing:
        return  # –ó–∞–ø–∏—Å –∑–Ω–∏–∫, —Å–∫—ñ–ø

    updates = []
    params = []

    fields = ['number', 'title', 'date', 'status', 'votes', 'url', 'author', 'text_length', 'has_answer']
    for i, field in enumerate(fields):
        if existing[i] is None and petition.get(field) is not None:
            updates.append(f"{field} = ?")
            params.append(petition.get(field))

    if updates:
        sql = f"UPDATE petitions SET {', '.join(updates)} WHERE source=? AND external_id=?"
        params.extend([petition['source'], petition['id']])
        con.execute(sql, params)


def backfill(start_id, end_id, test_mode=False):
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è backfill –∑ –º'—è–∫—à–∏–º —Å–∫—Ä–µ–π–ø—ñ–Ω–≥–æ–º"""

    print("="*70)
    print("üöÄ BACKFILL ARCHIVE PETITIONS (polite)")
    print("="*70)
    print(f"–î—ñ–∞–ø–∞–∑–æ–Ω: ID {start_id} ‚Üí {end_id}")
    print(f"–†–µ–∂–∏–º: {'TEST (–ø–µ—Ä—à—ñ 100)' if test_mode else 'PRODUCTION'}")
    print(f"–ß–∞—Å —Å—Ç–∞—Ä—Ç—É: {datetime.now().strftime('%H:%M:%S')}")
    print("="*70)

    con = duckdb.connect(DB_FILE)

    print("\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è existing IDs...")
    existing_ids = load_existing_ids(con)
    print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(existing_ids)} —ñ—Å–Ω—É—é—á–∏—Ö –∑–∞–ø–∏—Å—ñ–≤ –≤ –ë–î")

    stats = {
        'checked': 0,
        'found': 0,
        'inserted': 0,
        'updated': 0,
        'skipped_404': 0,
        'skipped_existing': 0
    }

    print(f"\nüîç –ü–æ—á–∏–Ω–∞—î–º–æ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è...\n")

    total = end_id - start_id + 1

    for pet_id in range(start_id, end_id + 1):
        stats['checked'] += 1

        # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ ID, —è–∫—ñ –≤–∂–µ —î –≤ –ë–î (–º—ñ–Ω—É—Å–∏–º –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è)
        if str(pet_id) in existing_ids:
            stats['skipped_existing'] += 1
            # –≤—Å–µ –æ–¥–Ω–æ —Ä–æ–±–∏–º–æ –Ω–µ–≤–µ–ª–∏–∫—É –ø–∞—É–∑—É, —â–æ–± –Ω–µ ¬´–ª–µ—Ç—ñ—Ç–∏¬ª –ø–æ —Ü–∏–∫–ª—É –Ω–∞–¥—Ç–æ —à–≤–∏–¥–∫–æ
            polite_sleep(stats['checked'])
            continue

        # Progress every 10
        if stats['checked'] % 10 == 0:
            found_rate = (stats['found'] / stats['checked']) * 100 if stats['checked'] > 0 else 0
            print(f"[{stats['checked']}/{total}] "
                  f"–ó–Ω–∞–π–¥–µ–Ω–æ: {stats['found']} ({found_rate:.1f}%) | "
                  f"–ù–æ–≤–∏—Ö: {stats['inserted']} | –û–Ω–æ–≤–ª–µ–Ω–∏—Ö: {stats['updated']} | "
                  f"–°–∫—ñ–ø (existing): {stats['skipped_existing']}")

        # Scrape
        data = extract_petition_data(pet_id)

        if not data:
            stats['skipped_404'] += 1
            polite_sleep(stats['checked'])
            continue

        stats['found'] += 1

        # –í–∏–∑–Ω–∞—á–∞—î–º–æ INSERT vs UPDATE (—Å—é–¥–∏ –ø–æ—Ç—Ä–∞–ø–ª—è—é—Ç—å —Ç—ñ–ª—å–∫–∏ –Ω–æ–≤—ñ ID,
        # –∞–ª–µ –∑–∞–ª–∏—à–∞—î–º–æ –≥–Ω—É—á–∫—ñ—Å—Ç—å)
        if data['id'] in existing_ids:
            update_existing(con, data)
            stats['updated'] += 1
        else:
            insert_new(con, data)
            stats['inserted'] += 1
            existing_ids.add(data['id'])

        # ¬´–õ—é–¥—Å—å–∫–∞¬ª –ø–∞—É–∑–∞
        polite_sleep(stats['checked'])

    con.close()

    print("\n" + "="*70)
    print("‚úÖ –ó–ê–í–ï–†–®–ï–ù–û!")
    print("="*70)
    print(f"–ü–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ ID:          {stats['checked']}")
    print(f"–ó–Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª—ñ–¥–Ω–∏—Ö:      {stats['found']} ({stats['found']/stats['checked']*100:.1f}%)")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ 404/–ø–æ–º–∏–ª–æ–∫:  {stats['skipped_404']}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ existing ID:  {stats['skipped_existing']}")
    print(f"–ù–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤:          {stats['inserted']}")
    print(f"–û–Ω–æ–≤–ª–µ–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤:      {stats['updated']}")
    print(f"–ß–∞—Å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è:         {datetime.now().strftime('%H:%M:%S')}")
    print("="*70)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Backfill archive petitions (polite)')
    parser.add_argument('--test', action='store_true', help='Test mode: only ID 1-100')
    parser.add_argument('--start', type=int, default=1000, help='Start ID')
    parser.add_argument('--end', type=int, default=200000, help='End ID')
    parser.add_argument('--full', action='store_true', help='Full range 1-200000')

    args = parser.parse_args()

    if args.test:
        backfill(1, 100, test_mode=True)
    elif args.full:
        backfill(1, 200000)
    else:
        backfill(args.start, args.end)
