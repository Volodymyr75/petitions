"""
–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è '–¥–æ–±–∏–≤–∞–Ω–Ω—è' –±–∞–∑–∏ –¥–∞–Ω–∏—Ö.
–¶—ñ–ª—å: 
1. –î–æ–¥–∞—Ç–∏ –ø–µ—Ç–∏—Ü—ñ—ó, —è–∫–∏—Ö –≤–∑–∞–≥–∞–ª—ñ –Ω–µ–º–∞—î –≤ –±–∞–∑—ñ (Missing IDs).
2. –û–Ω–æ–≤–∏—Ç–∏ –ø–µ—Ç–∏—Ü—ñ—ó, —è–∫—ñ —î –≤ –±–∞–∑—ñ, –∞–ª–µ –º–∞—é—Ç—å –ø–æ—Ä–æ–∂–Ω—ñ (NULL) –ø–æ–ª—è (author, text_length —Ç–æ—â–æ).
3. –ü—Ä–æ–ø—É—Å–∫–∞—Ç–∏ (–Ω–µ —Ä–æ–±–∏—Ç–∏ –∑–∞–ø–∏—Ç—ñ–≤) —Ç—ñ –ø–µ—Ç–∏—Ü—ñ—ó, —è–∫—ñ –≤–∂–µ –ø–æ–≤–Ω—ñ—Å—Ç—é –∑–∞–ø–æ–≤–Ω–µ–Ω—ñ.
"""
import requests
from bs4 import BeautifulSoup
import duckdb
import time
import random
import argparse
import re
from datetime import datetime

# Ukrainian month names to numbers
MONTHS_UA = {
    '—Å—ñ—á–Ω—è': 1, '–ª—é—Ç–æ–≥–æ': 2, '–±–µ—Ä–µ–∑–Ω—è': 3, '–∫–≤—ñ—Ç–Ω—è': 4,
    '—Ç—Ä–∞–≤–Ω—è': 5, '—á–µ—Ä–≤–Ω—è': 6, '–ª–∏–ø–Ω—è': 7, '—Å–µ—Ä–ø–Ω—è': 8,
    '–≤–µ—Ä–µ—Å–Ω—è': 9, '–∂–æ–≤—Ç–Ω—è': 10, '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': 11, '–≥—Ä—É–¥–Ω—è': 12
}

def normalize_date(date_str):
    """–ö–æ–Ω–≤–µ—Ä—Ç—É—î '15 –∂–æ–≤—Ç–Ω—è 2015' –∞–±–æ ISO ‚Üí 'YYYY-MM-DD'"""
    if not date_str:
        return None
    try:
        # President format: "15 –∂–æ–≤—Ç–Ω—è 2015"
        match = re.match(r'(\d{1,2})\s+(\w+)\s+(\d{4})', date_str)
        if match:
            day, month_ua, year = match.groups()
            month = MONTHS_UA.get(month_ua.lower())
            if month:
                return f"{year}-{month:02d}-{int(day):02d}"
        # Cabinet format: ISO "2021-12-02T00:00:00.000Z"
        if 'T' in date_str:
            return date_str[:10]
        return None
    except:
        return None

BASE_URL = "https://petition.president.gov.ua/petition/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
}
DB_FILE = "petitions.duckdb"

session = requests.Session()

def polite_sleep(iteration):
    time.sleep(random.uniform(0.7, 1.4))
    if iteration % 50 == 0:
        extra = random.uniform(3.0, 6.0)
        print(f"‚è≥ –ü–∞—É–∑–∞ {extra:.1f} —Å...")
        time.sleep(extra)

def extract_petition_data(pet_id, attempt=1, max_attempts=3):
    url = f"{BASE_URL}{pet_id}"
    try:
        resp = session.get(url, headers=HEADERS, timeout=15)
        if resp.status_code == 404:
            return None
        if resp.status_code in (429, 503):
            print(f"‚è≥ Rate limit {resp.status_code} –Ω–∞ ID {pet_id}, —á–µ–∫–∞—î–º–æ 30—Å")
            time.sleep(30)
            return extract_petition_data(pet_id, attempt, max_attempts)
        
        if resp.status_code != 200 or resp.url.endswith('/404'):
            return None

        soup = BeautifulSoup(resp.text, 'html.parser')
        h1 = soup.find('h1')
        if not h1 or "–¢–∞–∫–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–µ —ñ—Å–Ω—É—î" in h1.get_text():
            return None

        data = {
            'source': 'president',
            'id': str(pet_id),
            'title': h1.get_text(strip=True),
            'url': url
        }

        num_tag = soup.find(class_='pet_number')
        data['number'] = num_tag.get_text(strip=True) if num_tag else None

        date_tags = soup.find_all(class_='pet_date')
        data['author'] = None
        data['date'] = None
        for dt in date_tags:
            text = dt.get_text(strip=True)
            if "–ê–≤—Ç–æ—Ä" in text or "—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä" in text:
                data['author'] = text.split(":", 1)[1].strip() if ":" in text else text.replace("–ê–≤—Ç–æ—Ä (—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä)", "").strip()
            elif "–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è" in text:
                data['date'] = text.split(":", 1)[1].strip() if ":" in text else text.replace("–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è", "").strip()

        data['status'] = "Unknown"
        page_text = resp.text
        
        # –ü–æ—Ä—è–¥–æ–∫ –≤–∞–∂–ª–∏–≤–∏–π: –≤—ñ–¥ –Ω–∞–π–±—ñ–ª—å—à —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –¥–æ –∑–∞–≥–∞–ª—å–Ω–∏—Ö
        if "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é" in page_text: 
            data['status'] = "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é"
        elif "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ" in page_text: 
            data['status'] = "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ"
        elif "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤" in page_text or "–ó–∞–ª–∏—à–∏–ª–æ—Å—è" in page_text or "–ó–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤ —Ç—Ä–∏–≤–∞—î" in page_text:
            data['status'] = "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤"
        elif "–ù–µ –ø—ñ–¥—Ç—Ä–∏–º–∞–Ω–æ" in page_text:
            data['status'] = "–ù–µ –ø—ñ–¥—Ç—Ä–∏–º–∞–Ω–æ"
        elif "–ê—Ä—Ö—ñ–≤" in page_text: 
            data['status'] = "–ê—Ä—Ö—ñ–≤"

        votes_graph = soup.find(class_='petition_votes_graph')
        data['votes'] = int(votes_graph.get('data-votes', 0)) if votes_graph else None

        article = soup.find(class_='article')
        data['text_length'] = len(article.get_text()) if article else None
        data['has_answer'] = "–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–µ—Ç–∏—Ü—ñ—é" in page_text

        return data
    except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
        # Retry logic for timeout/connection errors
        if attempt < max_attempts:
            wait_time = 30 * attempt  # 30s, 60s, 90s
            print(f"‚è≥ Timeout –Ω–∞ ID {pet_id}, —Å–ø—Ä–æ–±–∞ {attempt}/{max_attempts}, —á–µ–∫–∞—î–º–æ {wait_time}—Å...")
            time.sleep(wait_time)
            return extract_petition_data(pet_id, attempt + 1, max_attempts)
        else:
            print(f"‚ùå ID {pet_id}: –≤—Å—ñ {max_attempts} —Å–ø—Ä–æ–± –Ω–µ–≤–¥–∞–ª—ñ, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ")
            return None
    except Exception as e:
        print(f"üí• –ü–æ–º–∏–ª–∫–∞ –Ω–∞ ID {pet_id}: {e}")
        return None

def get_work_lists(con):
    """–†–æ–∑–ø–æ–¥—ñ–ª—è—î ID –Ω–∞ —Ç—Ä–∏ –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó: –ü–æ–≤–Ω—ñ, –ü–æ—Ç—Ä–µ–±—É—é—Ç—å –æ–Ω–æ–≤–ª–µ–Ω–Ω—è, –í—ñ–¥—Å—É—Ç–Ω—ñ"""
    # 1. –¢—ñ, —â–æ –≤–∂–µ –º–∞—é—Ç—å –∞–≤—Ç–æ—Ä–∞ (–≤–≤–∞–∂–∞—î–º–æ —ó—Ö –ø–æ–≤–Ω–∏–º–∏)
    complete_ids = set([str(r[0]) for r in con.execute("SELECT external_id FROM petitions WHERE author IS NOT NULL AND source='president'").fetchall()])
    
    # 2. –¢—ñ, —â–æ –≤ –±–∞–∑—ñ, –∞–ª–µ –±–µ–∑ –∞–≤—Ç–æ—Ä–∞ (–ø–æ—Ç—Ä–µ–±—É—é—Ç—å –¥–æ–∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è)
    needs_update_ids = set([str(r[0]) for r in con.execute("SELECT external_id FROM petitions WHERE author IS NULL AND source='president'").fetchall()])
    
    return complete_ids, needs_update_ids

def backfill_smart(start_id, end_id):
    con = duckdb.connect(DB_FILE)
    complete_ids, needs_update_ids = get_work_lists(con)
    print(f"‚úÖ –í –±–∞–∑—ñ {len(complete_ids)} –∑–∞–ø–æ–≤–Ω–µ–Ω–∏—Ö –ø–µ—Ç–∏—Ü—ñ–π.")
    print(f"‚ö†Ô∏è {len(needs_update_ids)} –ø–µ—Ç–∏—Ü—ñ–π –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –¥–æ–∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è.")
    
    stats = {'checked': 0, 'inserted': 0, 'updated': 0, 'skipped': 0}
    
    for pet_id in range(start_id, end_id + 1):
        s_id = str(pet_id)
        stats['checked'] += 1
        
        # –°–¢–†–ê–¢–ï–ì–Ü–Ø –ü–†–û–ü–£–°–ö–£:
        # –Ø–∫—â–æ –ø–µ—Ç–∏—Ü—ñ—è –≤–∂–µ —î –≤ –±–∞–∑—ñ –Ü –≤–æ–Ω–∞ –ø–æ–≤–Ω–∞ (–º–∞—î –∞–≤—Ç–æ—Ä–∞) -> –ü–†–û–ü–£–°–ö–ê–Ñ–ú–û
        if s_id in complete_ids:
            continue
            
        # –Ø–∫—â–æ –º–∏ —Ç—É—Ç, –∑–Ω–∞—á–∏—Ç—å –ø–µ—Ç–∏—Ü—ñ—ó –∞–±–æ –Ω–µ–º–∞—î –≤ –±–∞–∑—ñ, –∞–±–æ –≤–æ–Ω–∞ –Ω–µ–ø–æ–≤–Ω–∞
        data = extract_petition_data(pet_id)
        
        if not data:
            stats['skipped'] += 1
        else:
            if s_id in needs_update_ids:
                # UPDATE
                date_norm = normalize_date(data.get('date'))
                fields = ['number', 'title', 'date', 'status', 'votes', 'url', 'author', 'text_length', 'has_answer']
                set_clause = ", ".join([f"{f} = ?" for f in fields])
                set_clause += ", date_normalized = ?"
                params = [data.get(f) for f in fields]
                params.append(date_norm)
                params.extend(['president', s_id])
                con.execute(f"UPDATE petitions SET {set_clause} WHERE source=? AND external_id=?", params)
                stats['updated'] += 1
            else:
                # INSERT
                date_norm = normalize_date(data.get('date'))
                con.execute("""
                    INSERT INTO petitions (source, external_id, number, title, date, status, votes, url, author, text_length, has_answer, date_normalized)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, ('president', s_id, data['number'], data['title'], data['date'], data['status'], data['votes'], data['url'], data['author'], data['text_length'], data['has_answer'], date_norm))
                stats['inserted'] += 1
        
        if stats['checked'] % 10 == 0:
            print(f"[{pet_id}] –û–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']} | –ù–æ–≤–∏—Ö: {stats['inserted']} | –ü—Ä–æ–ø—É—â–µ–Ω–æ (404): {stats['skipped']}")
        
        polite_sleep(stats['checked'])

    con.close()
    print(f"\n‚úÖ –ì–û–¢–û–í–û! –û–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, –î–æ–¥–∞–Ω–æ: {stats['inserted']}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--start', type=int, required=True)
    parser.add_argument('--end', type=int, required=True)
    args = parser.parse_args()
    backfill_smart(args.start, args.end)
