
import requests
from bs4 import BeautifulSoup
import time
import random
import re
from datetime import datetime

# --- CONFIG & HEADERS ---
BASE_URL = "https://petition.president.gov.ua/petition/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
}

# --- DATE HELPERS ---
MONTHS_UA = {
    '—Å—ñ—á–Ω—è': 1, '–ª—é—Ç–æ–≥–æ': 2, '–±–µ—Ä–µ–∑–Ω—è': 3, '–∫–≤—ñ—Ç–Ω—è': 4,
    '—Ç—Ä–∞–≤–Ω—è': 5, '—á–µ—Ä–≤–Ω—è': 6, '–ª–∏–ø–Ω—è': 7, '—Å–µ—Ä–ø–Ω—è': 8,
    '–≤–µ—Ä–µ—Å–Ω—è': 9, '–∂–æ–≤—Ç–Ω—è': 10, '–ª–∏—Å—Ç–æ–ø–∞–¥–∞': 11, '–≥—Ä—É–¥–Ω—è': 12
}

def normalize_date(date_str):
    """–ö–æ–Ω–≤–µ—Ä—Ç—É—î '15 –∂–æ–≤—Ç–Ω—è 2015' –∞–±–æ ISO ‚Üí 'YYYY-MM-DD'"""
    if not date_str:
        return None
    try:
        # President format: "15 –∂–æ–≤—Ç–Ω—è 2015"
        match = re.match(r'(\d{1,2})\s+(\w+)\s+(\d{4})', date_str)
        if match:
            day, month_ua, year = match.groups()
            month = MONTHS_UA.get(month_ua.lower())
            if month:
                return f"{year}-{month:02d}-{int(day):02d}"
        # Cabinet format: ISO "2021-12-02T00:00:00.000Z"
        if 'T' in date_str:
            return date_str[:10]
        return None
    except:
        return None

# --- PARSING HELPERS ---
def clean_votes(vote_str):
    if not vote_str:
        return 0
    digits = re.sub(r'\D', '', vote_str)
    return int(digits) if digits else 0

def extract_status(soup, page_text):
    """Determines status from page content"""
    # 1. Check classes first (more reliable)
    if soup.find(class_='status_active'): return "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤"
    if soup.find(class_='status_answered'): return "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é"
    if soup.find(class_='status_archive'): return "–ê—Ä—Ö—ñ–≤"
    if soup.find(class_='status_process'): return "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ"
    
    # 2. Text fallback
    if "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤" in page_text: return "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤"
    if "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ" in page_text: return "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ"
    if "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é" in page_text: return "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é"
    if "–ê—Ä—Ö—ñ–≤" in page_text: return "–ê—Ä—Ö—ñ–≤"
    
    return "Unknown"

# --- MAIN SCRAPER ---
def fetch_petition_detail(pet_id, session=None, attempt=1, max_attempts=3):
    """
    Fetches a single petition by ID.
    Returns dict or None if 404/Error.
    """
    if session is None:
        session = requests.Session()
        
    url = f"{BASE_URL}{pet_id}"
    
    try:
        resp = session.get(url, headers=HEADERS, timeout=15)
        
        # Handle 404 cleanly
        if resp.status_code == 404:
            return {"id": str(pet_id), "status": "Not Found", "error": 404}
            
        # Rate limits
        if resp.status_code in (429, 503):
            if attempt < max_attempts:
                wait_time = 30 * attempt
                print(f"‚è≥ Rate limit {resp.status_code} on ID {pet_id}, waiting {wait_time}s...")
                time.sleep(wait_time)
                return fetch_petition_detail(pet_id, session, attempt + 1, max_attempts)
            else:
                return {"id": str(pet_id), "error": resp.status_code}

        if resp.status_code != 200:
            return {"id": str(pet_id), "error": resp.status_code}
            
        # Parse
        soup = BeautifulSoup(resp.text, 'html.parser')
        h1 = soup.find('h1')
        
        if not h1 or "–¢–∞–∫–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–µ —ñ—Å–Ω—É—î" in h1.get_text():
            return {"id": str(pet_id), "status": "Not Found", "error": 404}

        data = {
            'source': 'president',
            'id': str(pet_id),
            'title': h1.get_text(strip=True),
            'url': url
        }

        # Number
        num_tag = soup.find(class_='pet_number')
        data['number'] = num_tag.get_text(strip=True) if num_tag else None

        # Dates & Author
        date_tags = soup.find_all(class_='pet_date')
        data['author'] = None
        data['date'] = None
        for dt in date_tags:
            text = dt.get_text(strip=True)
            if "–ê–≤—Ç–æ—Ä" in text or "—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä" in text:
                data['author'] = text.split(":", 1)[1].strip() if ":" in text else text.replace("–ê–≤—Ç–æ—Ä (—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä)", "").strip()
            elif "–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è" in text:
                data['date'] = text.split(":", 1)[1].strip() if ":" in text else text.replace("–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è", "").strip()

        # Status
        # 1. Try legacy class-based status
        # 2. Try new .petition_votes_status container
        # 3. Fallback to extracting from text
        status_text = extract_status(soup, resp.text)
        
        # Check new container if extract_status returned Unknown
        if status_text == "Unknown":
            new_status_div = soup.find(class_='petition_votes_status')
            if new_status_div:
                st_text = new_status_div.get_text(strip=True)
                if "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä" in st_text: status_text = "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤"
                elif "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ" in st_text: status_text = "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ"
                elif "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é" in st_text: status_text = "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é"
                elif "–ê—Ä—Ö—ñ–≤" in st_text: status_text = "–ê—Ä—Ö—ñ–≤"
        
        data['status'] = status_text

        # Votes
        votes_tag = soup.find(class_='pet_votes_num')
        if not votes_tag:
             votes_tag = soup.find(class_='pet_votes')
        
        # New structure support: votes are in .petition_votes_txt span
        if not votes_tag:
             # Find .petition_votes_txt and get the first span
             txt_div = soup.find(class_='petition_votes_txt')
             if txt_div:
                 votes_tag = txt_div.find('span')

        data['votes'] = clean_votes(votes_tag.get_text(strip=True)) if votes_tag else 0

        # Text length
        article = soup.find('article', class_='article')
        data['text_length'] = len(article.get_text(strip=True)) if article else 0
        
        # Legacy field (ignored but kept for schema)
        data['has_answer'] = (data['status'] == "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é")
        
        # Normalized date
        data['date_normalized'] = normalize_date(data.get('date'))

        return data

    except Exception as e:
        print(f"üí• Error scraping ID {pet_id}: {e}")
        return {"id": str(pet_id), "error": str(e)}
