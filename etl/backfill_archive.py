"""
Backfill script –¥–ª—è –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∞—Ä—Ö—ñ–≤–Ω–∏–º–∏ –ø–µ—Ç–∏—Ü—ñ—è–º–∏.

–õ–æ–≥—ñ–∫–∞:
- –Ø–∫—â–æ –∑–∞–ø–∏—Å –ù–û–í–ò–ô ‚Üí INSERT
- –Ø–∫—â–æ –∑–∞–ø–∏—Å –Ü–°–ù–£–Ñ ‚Üí UPDATE —Ç—ñ–ª—å–∫–∏ NULL –ø–æ–ª—è (–Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—É—î–º–æ —ñ—Å–Ω—É—é—á—ñ –¥–∞–Ω—ñ)

–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è:
- –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –≤—Å—ñ existing IDs –≤ –ø–∞–º'—è—Ç—å –æ–¥–∏–Ω —Ä–∞–∑ (—à–≤–∏–¥–∫–æ)
- –ë–∞—Ç—á—É–≤–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—ñ–≤ (20 –∑–∞–ø–∏—Å—ñ–≤ –∑–∞ —Ä–∞–∑)
- –ü—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä

–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
    python3 etl/backfill_archive.py --test        # –¢—ñ–ª—å–∫–∏ ID 1-100
    python3 etl/backfill_archive.py --start 1000 --end 10000
    python3 etl/backfill_archive.py --full        # –í–µ—Å—å –¥—ñ–∞–ø–∞–∑–æ–Ω 1-200000
"""
import requests
from bs4 import BeautifulSoup
import duckdb
import time
import random
import argparse
from datetime import datetime

BASE_URL = "https://petition.president.gov.ua/petition/"
HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"}
DB_FILE = "petitions.duckdb"

def extract_petition_data(pet_id):
    """–í–∏—Ç—è–≥—É—î –≤—Å—ñ –¥–æ—Å—Ç—É–ø–Ω—ñ –¥–∞–Ω—ñ –∑ –ø–µ—Ç–∏—Ü—ñ—ó"""
    url = f"{BASE_URL}{pet_id}"
    
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        
        if resp.status_code != 200:
            return None
            
        # Check for 404
        if "404" in resp.text or "–Ω–µ —ñ—Å–Ω—É—î" in resp.text or "Redirecting" in resp.text:
            return None
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # Title (–∫—Ä–∏—Ç–∏—á–Ω–æ)
        h1 = soup.find('h1')
        if not h1:
            return None
        
        data = {
            'source': 'president',
            'id': str(pet_id),
            'title': h1.get_text(strip=True)
        }
        
        # Number
        num_tag = soup.find(class_='pet_number')
        data['number'] = num_tag.get_text(strip=True) if num_tag else None
        
        # Date + Author
        date_tags = soup.find_all(class_='pet_date')
        data['author'] = None
        data['date'] = None
        
        for dt in date_tags:
            text = dt.get_text(strip=True)
            if "–ê–≤—Ç–æ—Ä" in text or "—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä" in text:
                if ":" in text:
                    data['author'] = text.split(":", 1)[1].strip()
                else:
                    data['author'] = text.replace("–ê–≤—Ç–æ—Ä (—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä)", "").strip()
            elif "–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è" in text:
                if ":" in text:
                    data['date'] = text.split(":", 1)[1].strip()
                else:
                    data['date'] = text.replace("–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è", "").strip()
        
        # Status
        data['status'] = "Unknown"
        if soup.find(string=lambda t: t and "–ê—Ä—Ö—ñ–≤" in t):
            data['status'] = "–ê—Ä—Ö—ñ–≤"
        elif soup.find(string=lambda t: t and "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é" in t):
            data['status'] = "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é"
        elif soup.find(string=lambda t: t and "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ" in t):
            data['status'] = "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ"
        elif soup.find(string=lambda t: t and "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤" in t):
            data['status'] = "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤"
        
        # Votes (–í–ê–ñ–õ–ò–í–û!)
        votes_graph = soup.find(class_='petition_votes_graph')
        if votes_graph:
            try:
                data['votes'] = int(votes_graph.get('data-votes', 0))
            except:
                data['votes'] = None
        else:
            data['votes'] = None
        
        # URL
        data['url'] = url
        
        # Text length
        article = soup.find(class_='article')
        data['text_length'] = len(article.get_text()) if article else None
        
        # Has answer
        answer_tab = soup.find(string=lambda t: t and "–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–µ—Ç–∏—Ü—ñ—é" in t)
        data['has_answer'] = answer_tab is not None
        
        return data
        
    except Exception as e:
        print(f"  üí• Error scraping {pet_id}: {e}")
        return None


def load_existing_ids(con):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤—Å—ñ existing petition IDs –≤ –ø–∞–º'—è—Ç—å (—à–≤–∏–¥–∫–æ)"""
    result = con.execute("""
        SELECT external_id FROM petitions WHERE source='president'
    """).fetchall()
    return set(row[0] for row in result)


def insert_new(con, petition):
    """INSERT –Ω–æ–≤–æ—ó –ø–µ—Ç–∏—Ü—ñ—ó"""
    con.execute("""
        INSERT INTO petitions (source, external_id, number, title, date, status, votes, url, author, text_length, has_answer)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        petition['source'],
        petition['id'],
        petition.get('number'),
        petition.get('title'),
        petition.get('date'),
        petition.get('status'),
        petition.get('votes'),
        petition.get('url'),
        petition.get('author'),
        petition.get('text_length'),
        petition.get('has_answer')
    ))


def update_existing(con, petition):
    """UPDATE —ñ—Å–Ω—É—é—á–æ—ó –ø–µ—Ç–∏—Ü—ñ—ó (—Ç—ñ–ª—å–∫–∏ NULL –ø–æ–ª—è)"""
    # –°–ø–æ—á–∞—Ç–∫—É –¥—ñ–∑–Ω–∞—î–º–æ—Å—è —è–∫—ñ –ø–æ–ª—è NULL
    existing = con.execute("""
        SELECT number, title, date, status, votes, url, author, text_length, has_answer
        FROM petitions
        WHERE source=? AND external_id=?
    """, (petition['source'], petition['id'])).fetchone()
    
    if not existing:
        return  # –ó–∞–ø–∏—Å –∑–Ω–∏–∫, —Å–∫—ñ–ø
    
    # UPDATE —Ç—ñ–ª—å–∫–∏ NULL –ø–æ–ª—è
    updates = []
    params = []
    
    fields = ['number', 'title', 'date', 'status', 'votes', 'url', 'author', 'text_length', 'has_answer']
    for i, field in enumerate(fields):
        if existing[i] is None and petition.get(field) is not None:
            updates.append(f"{field} = ?")
            params.append(petition.get(field))
    
    if updates:
        sql = f"UPDATE petitions SET {', '.join(updates)} WHERE source=? AND external_id=?"
        params.extend([petition['source'], petition['id']])
        con.execute(sql, params)


def backfill(start_id, end_id, test_mode=False):
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è backfill"""
    
    print("="*70)
    print("üöÄ BACKFILL ARCHIVE PETITIONS")
    print("="*70)
    print(f"–î—ñ–∞–ø–∞–∑–æ–Ω: ID {start_id} ‚Üí {end_id}")
    print(f"–†–µ–∂–∏–º: {'TEST (–ø–µ—Ä—à—ñ 100)' if test_mode else 'PRODUCTION'}")
    print(f"–ß–∞—Å —Å—Ç–∞—Ä—Ç—É: {datetime.now().strftime('%H:%M:%S')}")
    print("="*70)
    
    con = duckdb.connect(DB_FILE)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ existing IDs (–æ–¥–∏–Ω —Ä–∞–∑!)
    print("\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è existing IDs...")
    existing_ids = load_existing_ids(con)
    print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(existing_ids)} —ñ—Å–Ω—É—é—á–∏—Ö –∑–∞–ø–∏—Å—ñ–≤ –≤ –ë–î")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        'checked': 0,
        'found': 0,
        'inserted': 0,
        'updated': 0,
        'skipped_404': 0
    }
    
    batch = []
    
    print(f"\nüîç –ü–æ—á–∏–Ω–∞—î–º–æ —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è...\n")
    
    for pet_id in range(start_id, end_id + 1):
        stats['checked'] += 1

        # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ ID, —è–∫—ñ –≤–∂–µ —î –≤ –ë–î
        if str(pet_id) in existing_ids:
            continue
        
        # Progress every 10
        if stats['checked'] % 10 == 0:
            elapsed = stats['checked']
            found_rate = (stats['found'] / stats['checked']) * 100 if stats['checked'] > 0 else 0
            print(f"[{stats['checked']}/{end_id - start_id + 1}] "
                  f"–ó–Ω–∞–π–¥–µ–Ω–æ: {stats['found']} ({found_rate:.1f}%) | "
                  f"–ù–æ–≤–∏—Ö: {stats['inserted']} | –û–Ω–æ–≤–ª–µ–Ω–∏—Ö: {stats['updated']}")
        
        # Scrape
        data = extract_petition_data(pet_id)
        
        if not data:
            stats['skipped_404'] += 1
            continue
        
        stats['found'] += 1
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ INSERT vs UPDATE
        if data['id'] in existing_ids:
            update_existing(con, data)
            stats['updated'] += 1
        else:
            insert_new(con, data)
            stats['inserted'] += 1
            existing_ids.add(data['id'])  # –î–æ–¥–∞—î–º–æ –¥–æ –∫–µ—à—É
        
        # Polite delay
        time.sleep(random.uniform(0.3, 0.7))
    
    con.close()
    
    # Final report
    print("\n" + "="*70)
    print("‚úÖ –ó–ê–í–ï–†–®–ï–ù–û!")
    print("="*70)
    print(f"–ü–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ ID:     {stats['checked']}")
    print(f"–ó–Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª—ñ–¥–Ω–∏—Ö: {stats['found']} ({stats['found']/stats['checked']*100:.1f}%)")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ 404:     {stats['skipped_404']}")
    print(f"–ù–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤:     {stats['inserted']}")
    print(f"–û–Ω–æ–≤–ª–µ–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤: {stats['updated']}")
    print(f"–ß–∞—Å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è:    {datetime.now().strftime('%H:%M:%S')}")
    print("="*70)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Backfill archive petitions')
    parser.add_argument('--test', action='store_true', help='Test mode: only ID 1-100')
    parser.add_argument('--start', type=int, default=1000, help='Start ID')
    parser.add_argument('--end', type=int, default=200000, help='End ID')
    parser.add_argument('--full', action='store_true', help='Full range 1-200000')
    
    args = parser.parse_args()
    
    if args.test:
        backfill(1, 100, test_mode=True)
    elif args.full:
        backfill(1, 200000)
    else:
        backfill(args.start, args.end)
