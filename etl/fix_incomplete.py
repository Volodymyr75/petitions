"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–æ—á–∫–æ–≤–æ–≥–æ –¥–æ–∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö (author, text_length, has_answer).
–ë–µ—Ä–µ —Å–ø–∏—Å–æ–∫ ID –∑ —Ñ–∞–π–ª—É incomplete_petitions.json.
–í–∏–∫–æ–Ω—É—î —Ç—ñ–ª—å–∫–∏ UPDATE —ñ—Å–Ω—É—é—á–∏—Ö –∑–∞–ø–∏—Å—ñ–≤.
"""
import requests
from bs4 import BeautifulSoup
import duckdb
import time
import random
import json
from datetime import datetime

BASE_URL = "https://petition.president.gov.ua/petition/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
}
DB_FILE = "petitions.duckdb"
IDS_FILE = "incomplete_petitions.json"

session = requests.Session()

def polite_sleep(iteration):
    time.sleep(random.uniform(0.7, 1.4))
    if iteration % 50 == 0:
        extra = random.uniform(3.0, 6.0)
        print(f"‚è≥ –ü–∞—É–∑–∞ {extra:.1f} —Å...")
        time.sleep(extra)

def extract_petition_data(pet_id):
    url = f"{BASE_URL}{pet_id}"
    try:
        resp = session.get(url, headers=HEADERS, timeout=15)
        if resp.status_code == 404 or resp.url.endswith('/404'):
            return None
        if resp.status_code in (429, 503):
            print(f"‚è≥ Rate limit {resp.status_code} –Ω–∞ ID {pet_id}, —á–µ–∫–∞—î–º–æ 30—Å")
            time.sleep(30)
            return extract_petition_data(pet_id)
        
        if resp.status_code != 200:
            return None

        soup = BeautifulSoup(resp.text, 'html.parser')
        h1 = soup.find('h1')
        if not h1 or "–¢–∞–∫–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–µ —ñ—Å–Ω—É—î" in h1.get_text():
            return None

        data = {
            'source': 'president',
            'id': str(pet_id),
            'title': h1.get_text(strip=True),
            'url': url
        }

        num_tag = soup.find(class_='pet_number')
        data['number'] = num_tag.get_text(strip=True) if num_tag else None

        date_tags = soup.find_all(class_='pet_date')
        data['author'] = None
        data['date'] = None
        for dt in date_tags:
            text = dt.get_text(strip=True)
            if "–ê–≤—Ç–æ—Ä" in text or "—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä" in text:
                data['author'] = text.split(":", 1)[1].strip() if ":" in text else text.replace("–ê–≤—Ç–æ—Ä (—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä)", "").strip()
            elif "–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è" in text:
                data['date'] = text.split(":", 1)[1].strip() if ":" in text else text.replace("–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è", "").strip()

        page_text = resp.text
        data['status'] = "Unknown"
        
        # –ü–æ—Ä—è–¥–æ–∫ –≤–∞–∂–ª–∏–≤–∏–π: –≤—ñ–¥ –Ω–∞–π–±—ñ–ª—å—à —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–∏—Ö –¥–æ –∑–∞–≥–∞–ª—å–Ω–∏—Ö
        if "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é" in page_text: 
            data['status'] = "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é"
        elif "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ" in page_text: 
            data['status'] = "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ"
        elif "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤" in page_text or "–ó–∞–ª–∏—à–∏–ª–æ—Å—è" in page_text or "–ó–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤ —Ç—Ä–∏–≤–∞—î" in page_text:
            data['status'] = "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤"
        elif "–ù–µ –ø—ñ–¥—Ç—Ä–∏–º–∞–Ω–æ" in page_text:
            data['status'] = "–ù–µ –ø—ñ–¥—Ç—Ä–∏–º–∞–Ω–æ"
        elif "–ê—Ä—Ö—ñ–≤" in page_text: 
            data['status'] = "–ê—Ä—Ö—ñ–≤"

        votes_graph = soup.find(class_='petition_votes_graph')
        data['votes'] = int(votes_graph.get('data-votes', 0)) if votes_graph else None

        article = soup.find(class_='article')
        data['text_length'] = len(article.get_text()) if article else None
        data['has_answer'] = "–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–µ—Ç–∏—Ü—ñ—é" in page_text

        return data
    except Exception as e:
        print(f"üí• –ü–æ–º–∏–ª–∫–∞ –Ω–∞ ID {pet_id}: {e}")
        return None

def run_fix():
    print("="*70)
    print("üöÄ TARGETED BACKFILL FOR INCOMPLETE PETITIONS")
    print("="*70)

    try:
        with open(IDS_FILE, 'r') as f:
            ids_to_fix = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª {IDS_FILE} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ó–∞–ø—É—Å—Ç—ñ—Ç—å –∞—É–¥–∏—Ç —Å–ø–æ—á–∞—Ç–∫—É.")
        return

    total = len(ids_to_fix)
    print(f"üìã –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {total} ID –¥–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è.")
    
    con = duckdb.connect(DB_FILE)
    stats = {'checked': 0, 'updated': 0, 'skipped': 0}
    
    for pet_id in ids_to_fix:
        stats['checked'] += 1
        
        data = extract_petition_data(pet_id)
        
        if not data:
            # –Ø–∫—â–æ –ø–µ—Ç–∏—Ü—ñ—è —Ä–∞–ø—Ç–æ–º —Å—Ç–∞–ª–∞ 404, –∞–ª–µ –≤–æ–Ω–∞ –±—É–ª–∞ –≤ –±–∞–∑—ñ - –ø—Ä–æ—Å—Ç–æ —Å–∫—ñ–ø–∞—î–º–æ
            stats['skipped'] += 1
        else:
            # –í–∏–∫–æ–Ω—É—î–º–æ –ø–æ–≤–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–æ–ª—ñ–≤
            fields = ['number', 'title', 'date', 'status', 'votes', 'url', 'author', 'text_length', 'has_answer']
            set_clause = ", ".join([f"{f} = ?" for f in fields])
            params = [data.get(f) for f in fields]
            params.extend(['president', str(pet_id)])
            
            con.execute(f"UPDATE petitions SET {set_clause} WHERE source=? AND external_id=?", params)
            stats['updated'] += 1
        
        if stats['checked'] % 10 == 0:
            print(f"[{stats['checked']}/{total}] –û–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']} | –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']}")
        
        polite_sleep(stats['checked'])

    con.close()
    print(f"\n‚úÖ –ì–û–¢–û–í–û! –û–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']} –ø–µ—Ç–∏—Ü—ñ–π.")

if __name__ == "__main__":
    run_fix()
