import requests
from bs4 import BeautifulSoup
import duckdb
import time
import random
import argparse
from datetime import datetime

BASE_URL = "https://petition.president.gov.ua/petition/"
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ä–µ–∞–ª—å–Ω–∏–π User-Agent, —â–æ–± —Å–∞–π—Ç –Ω–µ –≤–≤–∞–∂–∞–≤ –Ω–∞—Å –ø—ñ–¥–æ–∑—Ä—ñ–ª–∏–º –±–æ—Ç–æ–º
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "uk-UA,uk;q=0.9,en-US;q=0.8,en;q=0.7",
}
DB_FILE = "petitions.duckdb"

# –ì–ª–æ–±–∞–ª—å–Ω–∞ —Å–µ—Å—ñ—è –¥–ª—è Keep-Alive –∑'—î–¥–Ω–∞–Ω—å
session = requests.Session()

def polite_sleep(iteration):
    """–ü–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–∞ —Å–µ—Ä–≤–µ—Ä"""
    time.sleep(random.uniform(0.7, 1.4))
    if iteration % 50 == 0:
        extra = random.uniform(3.0, 6.0)
        print(f"‚è≥ –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–∞—É–∑–∞ {extra:.1f} —Å...")
        time.sleep(extra)

def extract_petition_data(pet_id):
    """–í–∏—Ç—è–≥—É—î –¥–∞–Ω—ñ. –ü–æ–≤–µ—Ä—Ç–∞—î None —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ —Ä–µ–∞–ª—å–Ω–æ –Ω–µ —ñ—Å–Ω—É—î (404)"""
    url = f"{BASE_URL}{pet_id}"
    max_attempts = 3
    attempt = 0

    while attempt < max_attempts:
        try:
            resp = session.get(url, headers=HEADERS, timeout=15)

            # –Ø–∫—â–æ —Å–µ—Ä–≤–µ—Ä –ø–æ–≤–µ—Ä–Ω—É–≤ 404 ‚Äî –ø–µ—Ç–∏—Ü—ñ—ó —Ç–æ—á–Ω–æ –Ω–µ–º–∞—î
            if resp.status_code == 404:
                return None

            # –Ø–∫—â–æ –ª—ñ–º—ñ—Ç –∑–∞–ø–∏—Ç—ñ–≤ –∞–±–æ –ø–æ–º–∏–ª–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
            if resp.status_code in (429, 503):
                wait = 30 * (attempt + 1)
                print(f"‚è≥ Rate limit {resp.status_code} –Ω–∞ ID {pet_id}, —á–µ–∫–∞—î–º–æ {wait}s")
                time.sleep(wait)
                attempt += 1
                continue

            if resp.status_code != 200:
                return None

            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # –ì–æ–ª–æ–≤–Ω–∞ –æ–∑–Ω–∞–∫–∞ —ñ—Å–Ω—É—é—á–æ—ó –ø–µ—Ç–∏—Ü—ñ—ó ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫ h1
            h1 = soup.find('h1')
            if not h1:
                return None

            data = {
                'source': 'president',
                'id': str(pet_id),
                'title': h1.get_text(strip=True),
                'url': url
            }

            # –ù–æ–º–µ—Ä –ø–µ—Ç–∏—Ü—ñ—ó
            num_tag = soup.find(class_='pet_number')
            data['number'] = num_tag.get_text(strip=True) if num_tag else None

            # –î–∞—Ç–∞ —Ç–∞ –ê–≤—Ç–æ—Ä
            date_tags = soup.find_all(class_='pet_date')
            data['author'] = None
            data['date'] = None
            for dt in date_tags:
                text = dt.get_text(strip=True)
                if "–ê–≤—Ç–æ—Ä" in text or "—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä" in text:
                    data['author'] = text.split(":", 1)[1].strip() if ":" in text else text.replace("–ê–≤—Ç–æ—Ä (—ñ–Ω—ñ—Ü—ñ–∞—Ç–æ—Ä)", "").strip()
                elif "–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è" in text:
                    data['date'] = text.split(":", 1)[1].strip() if ":" in text else text.replace("–î–∞—Ç–∞ –æ–ø—Ä–∏–ª—é–¥–Ω–µ–Ω–Ω—è", "").strip()

            # –°—Ç–∞—Ç—É—Å
            data['status'] = "Unknown"
            # –®—É–∫–∞—î–º–æ —Å—Ç–∞—Ç—É—Å —É —Ç–µ–∫—Å—Ç—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
            page_text = resp.text
            if "–ê—Ä—Ö—ñ–≤" in page_text: data['status'] = "–ê—Ä—Ö—ñ–≤"
            elif "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é" in page_text: data['status'] = "–ó –≤—ñ–¥–ø–æ–≤—ñ–¥–¥—é"
            elif "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ" in page_text: data['status'] = "–ù–∞ —Ä–æ–∑–≥–ª—è–¥—ñ"
            elif "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤" in page_text: data['status'] = "–¢—Ä–∏–≤–∞—î –∑–±—ñ—Ä –ø—ñ–¥–ø–∏—Å—ñ–≤"

            # –ì–æ–ª–æ—Å–∏
            votes_graph = soup.find(class_='petition_votes_graph')
            data['votes'] = int(votes_graph.get('data-votes', 0)) if votes_graph else None

            # –¢–µ–∫—Å—Ç —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
            article = soup.find(class_='article')
            data['text_length'] = len(article.get_text()) if article else None
            data['has_answer'] = "–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–µ—Ç–∏—Ü—ñ—é" in page_text

            return data

        except Exception as e:
            attempt += 1
            print(f"üí• –ü–æ–º–∏–ª–∫–∞ –Ω–∞ ID {pet_id}: {e}, —Å–ø—Ä–æ–±–∞ {attempt}")
            time.sleep(5 * attempt)
    return None

def load_existing_ids(con):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —ñ—Å–Ω—É—é—á—ñ ID –¥–ª—è —à–≤–∏–¥–∫–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏"""
    result = con.execute("SELECT external_id FROM petitions WHERE source='president'").fetchall()
    return set(row[0] for row in result)

def insert_new(con, petition):
    """–î–æ–¥–∞–≤–∞–Ω–Ω—è –Ω–æ–≤–æ–≥–æ –∑–∞–ø–∏—Å—É"""
    con.execute("""
        INSERT INTO petitions (source, external_id, number, title, date, status, votes, url, author, text_length, has_answer)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (petition['source'], petition['id'], petition['number'], petition['title'], 
          petition['date'], petition['status'], petition['votes'], petition['url'], 
          petition['author'], petition['text_length'], petition['has_answer']))

def update_existing(con, petition):
    """–ü–æ–≤–Ω–µ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –í–°–Ü–• –ø–æ–ª—ñ–≤ —ñ—Å–Ω—É—é—á–æ–≥–æ –∑–∞–ø–∏—Å—É"""
    fields = ['number', 'title', 'date', 'status', 'votes', 'url', 'author', 'text_length', 'has_answer']
    set_clause = ", ".join([f"{f} = ?" for f in fields])
    params = [petition.get(f) for f in fields]
    
    # –î–æ–¥–∞—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è —É–º–æ–≤–∏ WHERE
    sql = f"UPDATE petitions SET {set_clause} WHERE source=? AND external_id=?"
    params.extend([petition['source'], petition['id']])
    con.execute(sql, params)

def backfill(start_id, end_id):
    """–ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–∞ –Ω–∞–ø–æ–≤–Ω–µ–Ω–Ω—è"""
    print("="*70)
    print("üöÄ PETITION UPDATER & BACKFILL (Safe Mode)")
    print(f"–î—ñ–∞–ø–∞–∑–æ–Ω: ID {start_id} ‚Üí {end_id}")
    print("="*70)

    con = duckdb.connect(DB_FILE)
    existing_ids = load_existing_ids(con)
    print(f"‚úÖ –í –±–∞–∑—ñ –≤–∂–µ —î {len(existing_ids)} –∑–∞–ø–∏—Å—ñ–≤.")

    stats = {'checked': 0, 'inserted': 0, 'updated': 0, 'skipped': 0}

    for pet_id in range(start_id, end_id + 1):
        stats['checked'] += 1
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Å–≤—ñ–∂—ñ –¥–∞–Ω—ñ –∑ —Å–∞–π—Ç—É
        data = extract_petition_data(pet_id)

        if not data:
            stats['skipped'] += 1
        else:
            if data['id'] in existing_ids:
                update_existing(con, data)
                stats['updated'] += 1
            else:
                insert_new(con, data)
                stats['inserted'] += 1
                existing_ids.add(data['id'])

        # –ó–≤—ñ—Ç –∫–æ–∂–Ω—ñ 10 ID
        if stats['checked'] % 10 == 0:
            print(f"[{pet_id}] –û–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']} | –ù–æ–≤–∏—Ö: {stats['inserted']} | –ü—Ä–æ–ø—É—â–µ–Ω–æ (404): {stats['skipped']}")

        polite_sleep(stats['checked'])

    con.close()
    print("\n" + "="*70)
    print(f"‚úÖ –ó–ê–í–ï–†–®–ï–ù–û! –û–Ω–æ–≤–ª–µ–Ω–æ: {stats['updated']}, –î–æ–¥–∞–Ω–æ –Ω–æ–≤–∏—Ö: {stats['inserted']}")
    print("="*70)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--start', type=int, required=True)
    parser.add_argument('--end', type=int, required=True)
    args = parser.parse_args()
    
    backfill(args.start, args.end)